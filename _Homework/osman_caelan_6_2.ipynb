{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Caelan Osman\n",
    "\n",
    "November 14, 2021\n",
    "\n",
    "Homework 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import  GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 6.6\n",
    "We prove the formulas (6.15), (6.18), and (6.19) in the text for training the naive Bayes classifer with maximum Likelihood estimators.\n",
    "We want to find the minimizer of\n",
    "\n",
    "\\begin{align}\n",
    "    &-\\sum_{c\\in\\mathscr{Y}} N_c \\log(\\pi_c) - \\sum_{j=1}^d\\sum_{c\\in\\mathscr{Y}}\\sum_{i:y_i = c} \\log(p(x_{ij}|\\boldsymbol{\\theta}_{j, c}))\\\\\n",
    "    &\\text{subject to} \\sum \\pi_c - 1 = 0 \\\\\n",
    "    &-\\pi_c \\leq 0\n",
    "\\end{align}\n",
    "\n",
    "Using the KKT conditions this leads the Lagrangian\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathscr{L} = &-\\sum_{c\\in\\mathscr{Y}} N_c \\log(\\pi_c) - \\sum_{j=1}^d\\sum_{c\\in\\mathscr{Y}}\\sum_{i:y_i = c} \\log(p(x_{ij}|\\boldsymbol{\\theta}_{j, c})) + \\lambda\\left(\\sum \\pi_c - 1\\right) - \\boldsymbol{\\mu}^T\\boldsymbol{\\pi}\n",
    "\\end{align}\n",
    "Taking derivatives tells us $\\boldsymbol{\\mu} = \\boldsymbol{0}$.\n",
    "Furthermore $\\frac{N_k}{\\pi_k} = \\lambda$ also that $\\lambda = N$ this gives us\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\pi}_c = \\frac{N_c}{N}.\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we want to find the argmin of the following\n",
    "\n",
    "\\begin{align}\n",
    "    \\min_{\\sigma^2, \\mu}\\sum_{i:y_i =c} \\log(2\\pi \\sigma^2) + \\frac{1}{\\sigma^2} (x_{ij} - \\mu)^2\n",
    "\\end{align}\n",
    "Taking the derivative with respect to $\\mu$ gives\n",
    "\n",
    "\\begin{align}\n",
    "    \\sum_{i:y_i = c} (x_{ij} - \\mu) = 0 \\implies \\widehat{\\mu}_{j, c} = \\frac{1}{N_c}\\sum_{i: y_i = c}x_{ij}\n",
    "\\end{align}\n",
    "\n",
    "Taking the derivative with respect to $\\sigma^2$ gives\n",
    "\n",
    "\\begin{align}\n",
    "    &\\sum \\frac1{\\sigma^2} + \\frac{1}{(\\sigma^2)^2}(x_{ij} -\\mu)^2 = 0\\\\\n",
    "    &\\sum \\frac1{\\sigma^2}\\left(1 - \\frac1{\\sigma^2}(x_{ij} - \\mu)^2    \\right) = 0\\\\\n",
    "    &\\implies N_c = \\frac{1}{\\sigma^2}\\sum(x_{ij} - \\mu)^2\\\\\n",
    "    &\\implies \\widehat{\\sigma^2}_{j, c} = \\frac{1}{N_c}\\sum_{i:y_i = c}(x_{ij} - \\mu_{j, c})^2\n",
    "\\end{align}\n",
    "as desired.\n",
    "\n",
    "Finally we have one more thing to minimize\n",
    "\\begin{align}\n",
    "    &\\sum - \\log\\left(\\theta_{j, c}^{x_j}(1 - \\theta_{j, c})^{1 - x_j}\\right)\\\\\n",
    "    &-\\sum  x_j\\log(\\theta_{j, c}) + (1 - x_j)\\log(1 - \\theta_{j, c})\\\\\n",
    "    &\\implies -\\sum \\frac{x_j}{\\theta_{j, c}} + \\sum \\frac{x_j}{\\theta_{j, c}} = 0\\\\\n",
    "    &\\implies \\sum \\frac{x_j}{\\theta_{j, c}} = \\sum\\frac{(1 - x_j)}{1 - \\theta_{j, c}}\\\\\n",
    "    &\\implies \\widehat{\\theta}_{j, c} = \\frac{N_{j, c}}{N_c}\n",
    "\\end{align}\n",
    "as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GNBC(object):\n",
    "    ''' This is a Gaussian naive Bayes classifier for\n",
    "        normally distributed features.\n",
    "        Paramters:\n",
    "            :param eps: the minimum variance\n",
    "        Methods:\n",
    "            :method __init__: the initizlizer function\n",
    "            :method fit_: the fitting function\n",
    "            :method predict_: the predict function\n",
    "    '''\n",
    "\n",
    "    def __init__(self, eps):\n",
    "        '''\n",
    "        :param eps: (float) the minimum variance\n",
    "        '''\n",
    "        self.eps = eps\n",
    "        return\n",
    "\n",
    "    def fit_(self,X, y):\n",
    "        '''\n",
    "        Parameters:\n",
    "            :param X: ((N, d), np.ndarray) training set, d is the number of features and\n",
    "                                           N is the number of data points\n",
    "            :param y: ((N, ), np.ndarray) the labels of the data points\n",
    "\n",
    "        Sets As Attributes:\n",
    "            :param n_features: (int) the total number of features\n",
    "            :param n_classes: (int) the total number of classes\n",
    "            :param class_counts: ((n_classes, ), np.ndarray) corresponds to the number N_c of\n",
    "                                                             training samples observed in each class c\n",
    "            :param pis: ((n_classes, ), np.ndarray) corresponding to the estimated probability Pi_c\n",
    "                                                    of each class c\n",
    "            :param classes: ((n_classes, ), np.ndarray) consisting of the class labels Y known to the classifier\n",
    "            :param sigmas: ((n_classes, n_features)) corresponding to the variance sigma_{j, c}^2 of each feature\n",
    "                                                     j per class c (also adding epsilon so no variance is ever zero)\n",
    "            :param mus:  ((n_classes, n_features)) corresponding to the mean mu_{j, c} of each feature j per class\n",
    "                                                   c.\n",
    "        '''\n",
    "        N, d = X.shape\n",
    "        #get a list of the unique_classes\n",
    "        self.n_classes = len(set(y))\n",
    "        #number of features is number of columns of X\n",
    "        self.n_features = d\n",
    "        #number of classes is the number of unique inputs in y\n",
    "        self.classes, self.class_counts = np.unique(y, return_counts=True)\n",
    "        #get pis\n",
    "        self.pis = self.class_counts / N\n",
    "        #now we calculate the mans and variances\n",
    "        self.sigmas = np.zeros((self.n_classes, self.n_features))\n",
    "        self.mus = np.zeros_like(self.sigmas)\n",
    "        for i in range(self.n_classes):\n",
    "            mask = y == self.classes[i]\n",
    "            for j in range(self.n_features):\n",
    "                self.mus[i, j] = np.mean(X[mask, j])\n",
    "                self.sigmas[i, j] = np.var(X[mask, j]) + self.eps\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict_(self, xs):\n",
    "        '''predicts the labels for the given array\n",
    "        :param x: ((d, ), np.ndarray): the values to predict for\n",
    "        :return: ((d, ), np.ndarray): the predictions given by the maximizer\n",
    "        '''\n",
    "        def scalar(x):\n",
    "            product = []\n",
    "            for c in range(self.n_classes):\n",
    "                p_x = np.exp(-(x - self.mus[c])**2/\n",
    "                             (2*self.sigmas[c])) \\\n",
    "                      /np.sqrt(2*np.pi*self.sigmas[c])\n",
    "                product.append(np.prod(p_x) * self.pis[c])\n",
    "            return np.argmax(product)\n",
    "        xs = np.atleast_1d(xs)\n",
    "        return np.array([scalar(x) for x in xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 6.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Code:\n",
      "time: 0.013268470764160156\n",
      "Misclassification rate: 0.022222222222222254\n",
      "\n",
      "Sklearn Naive Bayes:\n",
      "time: 0.010965347290039062\n",
      "Misclassification rate: 0.022222222222222254\n"
     ]
    }
   ],
   "source": [
    "def problem6_8():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    #part 1\n",
    "    X_train, X_test, y_train, y_test\\\n",
    "        = train_test_split(X, y, test_size = 0.3)\n",
    "    #part 2\n",
    "    start = time.time()\n",
    "    model = GNBC(1e-9)\n",
    "    model.fit_(X_train, y_train)\n",
    "    ans = model.predict_(X_test)\n",
    "    end = time.time()\n",
    "    print('My Code:')\n",
    "    print('time:', end-start)\n",
    "    mask = y_test == ans\n",
    "    print('Misclassification rate:', 1 - sum(mask)/ len(mask))\n",
    "\n",
    "    #part 3\n",
    "    start = time.time()\n",
    "    model = GaussianNB(var_smoothing=1e-9)\n",
    "    model.fit(X_train, y_train)\n",
    "    ans = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print()\n",
    "    print('Sklearn Naive Bayes:')\n",
    "    print('time:', end-start)\n",
    "    mask = y_test == ans\n",
    "    print('Misclassification rate:', 1 - sum(mask)/ len(mask))\n",
    "\n",
    "    return\n",
    "problem6_8()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Code:\n",
      "time: 0.53261399269104\n",
      "Misclassification rate: 0.21462639109697934\n",
      "\n",
      "Sklearn Naive Bayes:\n",
      "time: 0.008588790893554688\n",
      "Misclassification rate: 0.18044515103338632\n",
      "\n",
      "Sklearn KNN:\n",
      "time: 0.16542768478393555\n",
      "Misclassification rate: 0.027027027027026973\n"
     ]
    }
   ],
   "source": [
    "def problem6_9():\n",
    "    digits = datasets.load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "    #part 1\n",
    "    X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, train_size=0.3)\n",
    "    #part 2\n",
    "    start = time.time()\n",
    "    model = GNBC(1e-9)\n",
    "    model.fit_(X_train, y_train)\n",
    "    ans = model.predict_(X_test)\n",
    "    end = time.time()\n",
    "    print('My Code:')\n",
    "    print('time:', end-start)\n",
    "    mask = y_test == ans\n",
    "    print('Misclassification rate:', 1 - sum(mask)/ len(mask))\n",
    "\n",
    "    #part 3\n",
    "    start = time.time()\n",
    "    model = GaussianNB(var_smoothing=1e-9)\n",
    "    model.fit(X_train, y_train)\n",
    "    ans = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print()\n",
    "    print('Sklearn Naive Bayes:')\n",
    "    print('time:', end-start)\n",
    "    mask = y_test == ans\n",
    "    print('Misclassification rate:', 1 - sum(mask)/ len(mask))\n",
    "\n",
    "    start = time.time()\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    ans = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print()\n",
    "    print('Sklearn KNN:')\n",
    "    print('time:', end-start)\n",
    "    mask = y_test == ans\n",
    "    print('Misclassification rate:', 1 - sum(mask)/ len(mask))\n",
    "\n",
    "    return\n",
    "problem6_9()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}