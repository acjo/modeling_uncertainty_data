{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 7.12\n",
    "\n",
    "We find the minimizer of equation (7.7) in the book.\n",
    "\n",
    "\\begin{align*}\n",
    "0 &= \\frac\\partial{\\partial w_\\ell}\\tilde{J}(t) =\\frac\\partial{\\partial w_\\ell}\\left[ \\sum_{\\ell\\in L}\\left(G_\\ell w_\\ell + \\frac12(H_\\ell + \\lambda)w_\\ell^2\\right) + \\gamma|L|\\right] \\\\\n",
    "0&= G_\\ell +(H_\\ell + \\lambda)w_\\ell\\\\\n",
    "\\implies w_\\ell^* &= \\frac{-G_\\ell}{H_\\ell + \\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "Plugging this back into our formula gives us\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{J}(w_\\ell^*) &= \\sum_{\\ell\\in L}\\left( \\frac{-G_\\ell}{H_\\ell + \\lambda} G_\\ell + \\frac12(H_\\ell + \\lambda)\\left( \\frac{-G_\\ell}{H_\\ell + \\lambda}\\right)^2\\right) + \\gamma|L|\\\\\n",
    "                    &= \\sum_{\\ell\\in L}\\left( \\frac{-2G_\\ell^2}{2(H_\\ell + \\lambda)} +  \\frac{G_\\ell^2}{2(H_\\ell + \\lambda)}\\right) + \\gamma|L|\\\\\n",
    "                    &= \\sum_{\\ell\\in L}\\left( \\frac{-G_\\ell^2}{2(H_\\ell + \\lambda)} + \\gamma\\right) \\\\\n",
    "\\end{align*}\n",
    "as desired\n",
    "\n",
    "# Exercise 7.13\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{J}(t) &= \\sum_{\\ell \\in L} w_\\ell G_\\ell + \\frac{1}{2} \\sum_{\\ell \\in L} w_\\ell ^2H_\\ell + \\gamma |L| +  \\alpha \\sum_{\\ell\\in L} |w_\\ell|\n",
    "\\end{align*}\n",
    "There are 3 cases to consider for each leaf. \n",
    "1. $w_\\ell = 0$ then for this specific leaf we have that $\\tilde{J}_\\ell(t) =\\gamma$ where $\\tilde{J}_\\ell(t)$ denotes the objective function at leaf $\\ell$. Since this is now a constant objective function and $\\tilde{J}(t) = \\gamma |L|$ any $t, \\omega_\\ell$ is now a minimizer (and maximizer)\n",
    "\n",
    "2. $w_\\ell < 0$ then for this specific leaf we have that $\\tilde{J}_\\ell(t) = w_\\ell G_\\ell +\\frac12 w_\\ell^2 H_\\ell + \\gamma - \\alpha w_\\ell$ taking the derivative yields\n",
    "\\begin{align*}\n",
    "0 &= G_\\ell + w_\\ell H_\\ell - \\alpha\\\\\n",
    "w_\\ell &= \\frac{\\alpha - G_\\ell}{H_\\ell}\n",
    "\\end{align*}\n",
    "\n",
    "By the second derivative test, this is only a minimizer if  $(\\alpha - G_\\ell)/H_\\ell < 0$ which we cannot gurantee. \n",
    "\n",
    "\n",
    "3. $w_\\ell > 0$ then for this specific leaf we have $\\tilde{J}_\\ell(t) = w_\\ell G_\\ell + \\frac12 w_\\ell^2 H_\\ell + \\gamma + \\alpha w_\\ell$. Taking the derivative yields\n",
    "\n",
    "\\begin{align*}\n",
    "0 &= G_\\ell + w_\\ell H_\\ell + \\alpha \\\\\n",
    "w_\\ell &= -\\frac{G_\\ell + \\alpha}{H_\\ell}\n",
    "\\end{align*}\n",
    "Notice that this is always negative so we get that it is our minimizer. \n",
    "\n",
    "\n",
    "We are now looking for the contribution of each leaf to the objective function. \n",
    "To do this we plug our minimizer back into the objective function. \n",
    "\n",
    "\\begin{align*}\n",
    "J(w_\\ell^*) &= \\sum_{\\ell\\in L}-\\frac{G_\\ell + \\alpha}{H_\\ell} G_\\ell + \\frac12 \\left(-\\frac{G_\\ell + \\alpha}{H_\\ell}\\right)^2H_\\ell + \\gamma + \\alpha \\frac{G_\\ell + \\alpha}{H_\\ell}\\\\\n",
    "&= \\sum_{\\ell\\in L} \\gamma + (2\\alpha - 2 G_\\ell + (G_\\ell + \\alpha)) \\frac{G_\\ell + \\alpha}{2H_\\ell}\\\\  \n",
    "&= \\sum_{\\ell\\in L} \\gamma + (3\\alpha -  G_\\ell ) \\frac{G_\\ell + \\alpha}{2H_\\ell}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore the contribution of each leaf to the objective function is \n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{Cost}(\\ell) = \\gamma + (3\\alpha -  G_\\ell ) \\frac{G_\\ell + \\alpha}{2H_\\ell}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 08:49:32.338076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-06 08:49:32.338101: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7.14\n",
    "\n",
    "(i) Logistic regression is a poor choice for the fashion mnist dataset because this dataset is not binary. \n",
    "Additionally, there are 28^2 features which given the number of samples is too many to expect logistic regresison to perfrom well. To account for this we will use the mulit:softmax objective function as it can handle multiple classes\n",
    "\n",
    "(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:\n",
      "0.7948333333333333\n",
      "Best Parameters:\n",
      "{'eta': 4, 'gamma': 3, 'max_delta_step': 2, 'max_depth': 2, 'min_child_weight': 2, 'objective': 'multi:softmax'}\n"
     ]
    }
   ],
   "source": [
    "#create a random sample of 6000 points so we choose 600 points from all 10 labels\n",
    "mask0 = y_train == 0\n",
    "mask1 = y_train == 1\n",
    "mask2 = y_train == 2\n",
    "mask3 = y_train == 3\n",
    "mask4 = y_train == 4\n",
    "mask5 = y_train == 5\n",
    "mask6 = y_train == 6\n",
    "mask7 = y_train == 7\n",
    "mask8 = y_train == 8\n",
    "mask9 = y_train == 9\n",
    "\n",
    "random_choice = np.random.choice(np.arange(0, 6000), size=600)\n",
    "#get new x_train values\n",
    "x_train0 = x_train[mask0][random_choice]\n",
    "x_train1 = x_train[mask1][random_choice]\n",
    "x_train2 = x_train[mask2][random_choice]\n",
    "x_train3 = x_train[mask3][random_choice]\n",
    "x_train4 = x_train[mask4][random_choice]\n",
    "x_train5 = x_train[mask5][random_choice]\n",
    "x_train6 = x_train[mask6][random_choice]\n",
    "x_train7 = x_train[mask7][random_choice]\n",
    "x_train8 = x_train[mask8][random_choice]\n",
    "x_train9 = x_train[mask9][random_choice]\n",
    "#get new y train values\n",
    "y_train0 = y_train[mask0][random_choice]\n",
    "y_train1 = y_train[mask1][random_choice]\n",
    "y_train2 = y_train[mask2][random_choice]\n",
    "y_train3 = y_train[mask3][random_choice]\n",
    "y_train4 = y_train[mask4][random_choice]\n",
    "y_train5 = y_train[mask5][random_choice]\n",
    "y_train6 = y_train[mask6][random_choice]\n",
    "y_train7 = y_train[mask7][random_choice]\n",
    "y_train8 = y_train[mask8][random_choice]\n",
    "y_train9 = y_train[mask9][random_choice]\n",
    "\n",
    "\n",
    "X_train = np.concatenate((x_train0, x_train1, x_train2, \n",
    "                        x_train3, x_train4, x_train5, \n",
    "                        x_train6, x_train7, x_train8, \n",
    "                        x_train9))\n",
    "Y_train = np.concatenate((y_train0, y_train1, y_train2, \n",
    "                          y_train3, y_train4, y_train5, \n",
    "                          y_train6, y_train7, y_train8, \n",
    "                          y_train9))\n",
    "# gamma = 0.25\n",
    "# eta = 0.3\n",
    "# obj= 'mulit:softmax'\n",
    "boost = xgb.XGBClassifier(verbosity = 0)\n",
    "\n",
    "params = { 'gamma':[3], 'eta':[4], \n",
    "          'objective':['multi:softmax'],\n",
    "          'max_depth': np.arange(1, 3), \n",
    "          'min_child_weight':np.arange(1, 3), \n",
    "          'max_delta_step':np.arange(1, 3)\n",
    "         }\n",
    "\n",
    "GS = GridSearchCV(estimator=boost, param_grid=params, n_jobs=6, cv=2)\n",
    "GS.fit([x.ravel() for x in X_train], Y_train)\n",
    "best_params = GS.best_params_\n",
    "best_score = GS.best_score_\n",
    "print('Best Score:', best_score, sep='\\n')\n",
    "print('Best Parameters:', best_params, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest training time:\n",
      "4.78 s ± 193 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "random forest score:\n",
      "0.8359\n",
      "boosted tree training time:\n",
      "24.7 s ± 1.73 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "boosted tree score:\n",
      "0.6952\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "print('random forest training time:')\n",
    "%timeit RF.fit([x.ravel() for x in X_train], Y_train)\n",
    "print('random forest score:', RF.score([x.ravel() for x in x_test], y_test), sep='\\n')\n",
    "\n",
    "boost = xgb.XGBClassifier(verbosity=0, gamma=3, eta=4, objective='multi:softmax', \n",
    "                          max_depth=best_params['max_depth'], \n",
    "                          min_child_weight=best_params['min_child_weight'], \n",
    "                          max_delta_step=best_params['max_delta_step'])\n",
    "print('boosted tree training time:')\n",
    "%timeit boost.fit([x.ravel() for x in X_train], Y_train)\n",
    "print('boosted tree score:', boost.score([x.ravel() for x in x_test], y_test), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15\n",
    "\n",
    "I won't use my portfoilio data since the project was already due and my data takes quite a bit of code to properly clean. Instead I will use the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:\n",
      "0.8543543432\n",
      "Best Parameters:\n",
      "{'alpha':2, 'gamma':1, 'lambda':1, 'eta':2}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "mask0 = y_train == 0\n",
    "mask1 = y_train == 1\n",
    "mask2 = y_train == 2\n",
    "mask3 = y_train == 3\n",
    "mask4 = y_train == 4\n",
    "mask5 = y_train == 5\n",
    "mask6 = y_train == 6\n",
    "mask7 = y_train == 7\n",
    "mask8 = y_train == 8\n",
    "mask9 = y_train == 9\n",
    "\n",
    "random_choice = np.random.choice(np.arange(0, 5000), size=600)\n",
    "#get new x_train values\n",
    "x_train0 = x_train[mask0][random_choice]\n",
    "x_train1 = x_train[mask1][random_choice]\n",
    "x_train2 = x_train[mask2][random_choice]\n",
    "x_train3 = x_train[mask3][random_choice]\n",
    "x_train4 = x_train[mask4][random_choice]\n",
    "x_train5 = x_train[mask5][random_choice]\n",
    "x_train6 = x_train[mask6][random_choice]\n",
    "x_train7 = x_train[mask7][random_choice]\n",
    "x_train8 = x_train[mask8][random_choice]\n",
    "x_train9 = x_train[mask9][random_choice]\n",
    "#get new y train values\n",
    "y_train0 = y_train[mask0][random_choice]\n",
    "y_train1 = y_train[mask1][random_choice]\n",
    "y_train2 = y_train[mask2][random_choice]\n",
    "y_train3 = y_train[mask3][random_choice]\n",
    "y_train4 = y_train[mask4][random_choice]\n",
    "y_train5 = y_train[mask5][random_choice]\n",
    "y_train6 = y_train[mask6][random_choice]\n",
    "y_train7 = y_train[mask7][random_choice]\n",
    "y_train8 = y_train[mask8][random_choice]\n",
    "y_train9 = y_train[mask9][random_choice]\n",
    "\n",
    "\n",
    "X_train = np.concatenate((x_train0, x_train1, x_train2, \n",
    "                        x_train3, x_train4, x_train5, \n",
    "                        x_train6, x_train7, x_train8, \n",
    "                        x_train9))\n",
    "Y_train = np.concatenate((y_train0, y_train1, y_train2, \n",
    "                          y_train3, y_train4, y_train5, \n",
    "                          y_train6, y_train7, y_train8, \n",
    "                          y_train9))\n",
    "\n",
    "boost = xgb.XGBClassifier(verbosity = 0, objective='multi:softmax')\n",
    "\n",
    "params = {'alpha':[1, 2], 'gamma':[1, 2], 'lambda':[1,2], 'eta':[1, 2]}\n",
    "GS = GridSearchCV(estimator=boost, param_grid=params, n_jobs=6, cv=2)\n",
    "GS.fit([x.ravel() for x in X_train], Y_train)\n",
    "best_params = GS.best_params_\n",
    "best_score = GS.best_score_\n",
    "print('Best Score:', best_score, sep='\\n')\n",
    "print('Best Parameters:', best_params, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}