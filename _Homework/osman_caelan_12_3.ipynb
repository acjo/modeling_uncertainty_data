{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb7b49a",
   "metadata": {},
   "source": [
    "# Exercise 12.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ca40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import linalg as la\n",
    "from scipy.stats import multivariate_normal as mn\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8cf8a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_GMM(zs, K, w0s=None, mu0s =None, Sigma0s = None, tol=1e-20, maxiter=200):\n",
    "    \n",
    "    # if the parameters aren't initialized, intialize them\n",
    "    if w0s is None:\n",
    "        w0s = np.random.uniform(0, 1, size = K)\n",
    "        w0s /= la.norm(w0s, ord=1)\n",
    "    if mu0s is None:\n",
    "        indices = np.arange(0, zs.shape[0])\n",
    "        choice = np.random.choice(indices, size=K, replace=False)\n",
    "        mu0s = zs[choice]\n",
    "    if Sigma0s is None:\n",
    "        Sigma0s = np.array([np.eye(zs[0].size) for _ in range(K)])\n",
    "        \n",
    "    # helper function for computing q_i^t(k)\n",
    "    def q(m, h):\n",
    "        num = np.exp(-np.inner(zs[m] - mu0s[h], la.inv(Sigma0s[h])@(zs[m] - mu0s[h]))/2)*w0s[h]\\\n",
    "                /np.sqrt(la.det(Sigma0s[h]))\n",
    "        \n",
    "        denom = np.sum([np.exp(-np.inner((zs[m] - mu0s[hp]), la.inv(Sigma0s[hp])@(zs[m]-mu0s[hp]))/2)*w0s[hp]\n",
    "                        /np.sqrt(la.det(Sigma0s[hp]))\n",
    "                        for hp in range(K)])\n",
    "        \n",
    "        return num / denom\n",
    "\n",
    "    # now we update\n",
    "    n = zs.shape[0]\n",
    "    for j in range(maxiter):\n",
    "        # initalize updates\n",
    "        w1s = np.empty_like(w0s)\n",
    "        mu1s = np.empty_like(mu0s)\n",
    "        Sigma1s = np.empty_like(Sigma0s)\n",
    "\n",
    "        # populate the updates\n",
    "        for k in range(K):\n",
    "            #get current Q vector\n",
    "            curr_Q = np.array([q(i, k) for i in range(n)])\n",
    "            # vupdate weight\n",
    "            w_update = np.sum(curr_Q) / n\n",
    "            w1s[k] = w_update\n",
    "            # update mu\n",
    "            mu_update = np.sum(curr_Q.reshape(-1, 1) * zs, axis= 0) / (n*w_update)\n",
    "            mu1s[k] = mu_update\n",
    "            # update Sigma\n",
    "            Sigma_update = curr_Q * (zs - mu_update).T @ (zs - mu_update) / (n*w_update)\n",
    "            Sigma1s[k] = Sigma_update\n",
    "            \n",
    "        # check convergence\n",
    "        if la.norm(w0s - w1s, ord=1) < tol:\n",
    "            \n",
    "            return w1s, mu1s, Sigma1s, True, j+1\n",
    "\n",
    "        # if not converged, assign updates\n",
    "        w0s = w1s.copy()\n",
    "        mu0s = mu1s.copy()\n",
    "        Sigma0s = Sigma1s.copy()\n",
    "        \n",
    "    return w0s, mu0s, Sigma0s, False, j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab53f1",
   "metadata": {},
   "source": [
    "## Exercise 12.10 \n",
    "\n",
    "### part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e23c55ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM for GMM accuracy:  0.9666666666666667\n",
      "KMeans Algo: 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "def problem12_10_1():\n",
    "    \n",
    "    iris = datasets.load_iris()\n",
    "    \n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    weights, mus, Sigmas, converged, iterations = EM_GMM(X, 3)\n",
    "    \n",
    "    \n",
    "    classification = np.empty(X.shape[0])\n",
    "    for i, x in enumerate(X):\n",
    "        curr_class = np.array([mn.pdf(x, mean=mus[k], cov=Sigmas[k]) for k in range(3)])\n",
    "        classification[i] = np.argmax(curr_class)\n",
    "        \n",
    "    print('EM for GMM accuracy: ', np.sum(classification == y) / y.size)\n",
    "        \n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "    KM = KMeans(n_clusters=3)\n",
    "    KM.fit(X_train)\n",
    "    prediction = KM.predict(X_test)\n",
    "    print('KMeans Algo:', np.sum(prediction == y_test)/y_test.size)\n",
    "    return\n",
    "\n",
    "problem12_10_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d3c3b",
   "metadata": {},
   "source": [
    "### part 2\n",
    "\n",
    "We will use the titanic dataset file used in the KMeans lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8496a6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM for GMM accuracy:  0.4426229508196721\n",
      "KMeans Algo: 0.37158469945355194\n"
     ]
    }
   ],
   "source": [
    "def problem12_10_2():\n",
    "    \n",
    "    X = pd.read_csv('train.csv')\n",
    "    #drop na\n",
    "    X.dropna(inplace=True)\n",
    "    #get label\n",
    "    y = X['Survived']\n",
    "    #drop columns that aren't filled or will affect the classifier\n",
    "    X = X.drop(['Cabin', 'Name', 'PassengerId', 'Survived', 'Ticket'], axis=1)\n",
    "    #one-hot-encode\n",
    "    X = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "    \n",
    "     \n",
    "    weights, mus, Sigmas, converged, iterations = EM_GMM(X.values, 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    classification = np.empty(X.values.shape[0])\n",
    "    for i, x in enumerate(X.values):\n",
    "        curr_class = np.array([mn.pdf(x, mean=mus[k], cov=Sigmas[k]) for k in range(2)])\n",
    "        classification[i] = np.argmax(curr_class)\n",
    "        \n",
    "    print('EM for GMM accuracy: ', np.sum(classification == y.values) / y.values.size)\n",
    "        \n",
    "        \n",
    "    KM = KMeans(n_clusters=2)\n",
    "    KM.fit(X)\n",
    "    prediction = KM.predict(X)\n",
    "    print('KMeans Algo:', np.sum(prediction == y)/y.size)\n",
    "    return\n",
    "\n",
    "\n",
    "problem12_10_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd307d",
   "metadata": {},
   "source": [
    "Because we are using the titanic dataset the only clustering that really makese sense is 2. (whethere they survived or not). As we can see, neither the KMeans algorithm provided by sklearn or the EM for GMM algorithm I designed are terribly inaccurate. Although, mine is certainly more accurate than than KMeans. This is most likely because the assumptions made in the KMeans implementation are not valid, i.e. the $w_k$ are not all equal and the covariance matrices $\\Sigma_k$ are not equal and diagonal. The GMM model may not be that accurate because the assumption that there is a Gaussian distribution underlying the data is probably erroneous. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3811jvsc74a57bd0a3787f4cb79ecec69b787db7199d8bde05c4992db9bd29a2a965f7beb5defefb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
