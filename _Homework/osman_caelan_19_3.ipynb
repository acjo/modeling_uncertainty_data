{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkfOwBlRaXZ9"
   },
   "source": [
    "# Caelan Osman\n",
    "# Homework 19.3\n",
    "# March 31, 2022\n",
    "\n",
    "## Exercise 19.7\n",
    "\n",
    "As we can see, the model we had with the largest parameters (from the last homework) was the model with 1.57 million parameters given by layers with widths [1000, 500, 300, 200, 100].\n",
    "We rebuild that model here and experiment with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F50A9t52Q_Ab",
    "outputId": "d4d822c0-b2b2-428d-8daf-aa57a9397e7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yrr1xqbCQ_Ae"
   },
   "outputs": [],
   "source": [
    "from keras.backend import batch_normalization\n",
    "########################\n",
    "# HELPER CODE FOR TRAINING\n",
    "########################\n",
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    This function prepares the data for training\n",
    "    \"\"\"\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    # Rescale the inputs to be in the interval [-0.5,0.5], \n",
    "    X_train_full = X_train_full/255 - 0.5  # Rescale the training set input\n",
    "    X_test = X_test/255  - 0.5   # Apply the same transform to the test set\n",
    "\n",
    "    # make a validation split beyond the basic test/train\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full,\n",
    "                                                      test_size = 0.2, \n",
    "                                                      random_state = 42)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, X_test, y_test\n",
    "\n",
    "   \n",
    "def build_model(widths=[], l2=0, dropout=0, batch = False):\n",
    "    \"\"\"\n",
    "    This function builds a model with the given widths for the dense layers\n",
    "    \"\"\"\n",
    "\n",
    "    if dropout != 0 and batch:\n",
    "        raise ValueError('Cannot apply batch and dropout at the same time')\n",
    "\n",
    "\n",
    "    # add initial layers\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    \n",
    "\n",
    "    # add as many dense layers as we want\n",
    "    for i, w in enumerate(widths):\n",
    "        \n",
    "        # add regularization\n",
    "        if l2 != 0:\n",
    "            model.add(keras.layers.Dense(w, activation=\"relu\",\n",
    "                                         activity_regularizer = tf.keras.regularizers.l2(l2)))\n",
    "        else:\n",
    "            model.add(keras.layers.Dense(w, activation=\"relu\"))\n",
    "\n",
    "        # add dropout model\n",
    "        if dropout != 0 and i == 2:\n",
    "            model.add(tf.keras.layers.Dropout(dropout, input_shape = (w, )))\n",
    "\n",
    "\n",
    "\n",
    "    if batch:\n",
    "        model.add(tf.keras.layers.BatchNormalization(\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=0.001,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=\"zeros\",\n",
    "            gamma_initializer=\"ones\",\n",
    "            moving_mean_initializer=\"zeros\",\n",
    "            moving_variance_initializer=\"ones\",\n",
    "            beta_regularizer=None,\n",
    "            gamma_regularizer=None,\n",
    "            beta_constraint=None,\n",
    "            gamma_constraint=None\n",
    "            ))\n",
    "\n",
    "    # add softmax layer\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "\n",
    "    return model\n",
    "    \n",
    "def set_optimizer(learning_rate=0.2, decay=1e-4, momentum=0.8, nesterov=False, Adam=False):\n",
    "    \"\"\"\n",
    "    This function returns the appropriate optimizer\n",
    "    \"\"\"\n",
    "    if Adam:\n",
    "        return keras.optimizers.Adam(lr=learning_rate)\n",
    "    else:\n",
    "        ## Learning rate with decay\n",
    "        ## decay works as   lr *= (1. / (1. + decay * iterations))\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate = learning_rate, \n",
    "                                      decay=decay, \n",
    "                                      momentum=momentum, \n",
    "                                      nesterov=nesterov)\n",
    "        \n",
    "        return opt\n",
    "    \n",
    "def train_models(model, opt, X_train, y_train, X_val, y_val,  batch_size=32, epochs=12, verbose=0):\n",
    "    \"\"\"\n",
    "    This function trains the model\n",
    "    \"\"\"\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=opt,\n",
    "                      metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=verbose,\n",
    "                    validation_data=(X_val, y_val)\n",
    "                   )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def plot_history(history, title=\"\"):\n",
    "    \"\"\"\n",
    "    This function plots the history\n",
    "    \"\"\"\n",
    "    # Plot the history\n",
    "    plt.figure(figsize=(8,4),dpi=100)\n",
    "\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    ax.plot(history.history['accuracy'], label='training accuracy')\n",
    "    ax.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.legend(frameon=False)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    ax.plot(history.history['loss'], label='training loss')\n",
    "    ax.plot(history.history['val_loss'], label='validation loss')\n",
    "    ax.legend(frameon=False)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    #plt.ylim(0,1);\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKbrDwsNcoW-",
    "outputId": "678f5087-d053-43a3-80ed-30bb3ebb62a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3458 - accuracy: 0.8856\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3967 - accuracy: 0.8899\n",
      "375/375 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.1002\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4514 - accuracy: 0.8635\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4046 - accuracy: 0.8714\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3987 - accuracy: 0.8774\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3056 - accuracy: 0.1002\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3037 - accuracy: 0.1004\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3027 - accuracy: 0.0966\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3138 - accuracy: 0.8898\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3918 - accuracy: 0.8989\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3964 - accuracy: 0.8976\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3400 - accuracy: 0.8884\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3029 - accuracy: 0.1009\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.0966\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3374 - accuracy: 0.8863\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3676 - accuracy: 0.8958\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3845 - accuracy: 0.8943\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3793 - accuracy: 0.8699\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.8295\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3624 - accuracy: 0.8802\n"
     ]
    }
   ],
   "source": [
    "# get best model\n",
    "def get_best_model():\n",
    "\n",
    "    # get training, validat, and testing data \n",
    "    X_train, X_val, y_train, y_val, X_test, y_test = prepare_data()\n",
    "\n",
    "    # initialization best validation score\n",
    "    best_val_score = -np.inf\n",
    "\n",
    "    # define layers\n",
    "    layers = [1000, 500, 300, 200, 100]\n",
    "    # l2, dropout, batch parameters\n",
    "    l2_params = [1e-4, 1e-3, 1e-2, 0, 0, 0, 0]\n",
    "    dropout_params = [0.05, 0.01, 0, 0.1, 0.25, 0.25, 0]\n",
    "    batch_params = [False, False, True, False, False, False, True]\n",
    "\n",
    "    # P, LR, DR paramters\n",
    "    P = [0.85, 0.8, 0.65]\n",
    "    LR = [0.1, 0.2, 0.3]\n",
    "    DR = [1e-4, 1e-3, 1e-2] \n",
    "\n",
    "    for l2, drop, batch in zip(l2_params, dropout_params, batch_params):\n",
    "        \n",
    "        # get model\n",
    "        model = build_model(widths=layers, l2=l2, dropout=drop, batch=batch  )\n",
    "\n",
    "        for p, lr, dr in zip(P, LR, DR):\n",
    "            # get optimizer\n",
    "            opt = set_optimizer(learning_rate=lr, decay=dr, momentum=p)\n",
    "\n",
    "            # train model\n",
    "            model, _ = train_models(model, opt, X_train, y_train, X_val, y_val)\n",
    "\n",
    "            # get validation accuracy\n",
    "            _, accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "            # now save the best model \n",
    "            if accuracy > best_val_score:\n",
    "                best_params = {\"layers\": layers, \"l2\" : l2, \"drop\" :drop, \n",
    "                               \"batch\" : batch, \"p\" :p, \n",
    "                               \"lr\" :lr, \"dr\": dr} \n",
    "                best_val_score = accuracy\n",
    "                best_model = model\n",
    "\n",
    "    return best_model, best_params, best_val_score\n",
    "\n",
    "best_model, best_params, best_val_score = get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87MdMK8sutHU",
    "outputId": "38ed956f-d76d-4f64-b970-7aa38ee60d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4219 - accuracy: 0.8956\n",
      "Best Hyper Parameters:\n",
      "layers: [1000, 500, 300, 200, 100]\n",
      "l2: 0\n",
      "drop: 0.1\n",
      "batch: False\n",
      "p: 0.8\n",
      "lr: 0.2\n",
      "dr: 0.001\n",
      "Best model Test Score:  0.8956000208854675\n",
      "Best model Loss:  0.4219379425048828\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1000)              785000    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 300)               150300    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,517,110\n",
      "Trainable params: 1,517,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, X_test, y_test = prepare_data()\n",
    "\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test) \n",
    "\n",
    "print('Best Hyper Parameters:')\n",
    "for key in list(best_params.keys()):\n",
    "    print(key, best_params[key], sep=\": \")\n",
    "\n",
    "print('Best model Test Score: ', accuracy)\n",
    "print('Best model Loss: ', loss)\n",
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
