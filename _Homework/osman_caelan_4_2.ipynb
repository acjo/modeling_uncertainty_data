{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import numpy as anp \n",
    "from scipy.special import digamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.10 (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mle(x):\n",
    "    ''' Computes the MLE for (p, k) of the negative binomial distribution given a \n",
    "        draw x.\n",
    "        Parameters:\n",
    "            x ((n, ) np.ndarray) \n",
    "        Reutnrs:\n",
    "            k ( int ): a positive integer for the estimate of k\n",
    "            p ( float ): a positve value in [0, 1] as the estimate for p \n",
    "    '''\n",
    "\n",
    "    def sample_mean_estimator(x):\n",
    "        ''' Returns the value of the sample mean\n",
    "            Paramters:\n",
    "                x ((n, ) np.ndarray): the sample\n",
    "            Retunrs: \n",
    "                μ: (float): the sample mean\n",
    "        '''\n",
    "        n = x.size\n",
    "        return np.sum(x) / n\n",
    "\n",
    "    def p_mom(x):\n",
    "        ''' Returns the method of moments estimator for p\n",
    "            Paramters:\n",
    "                x ((n, ) np.ndarray): the sample\n",
    "            Retunrs: \n",
    "                p_mom: (float): the method of moments estimator for p\n",
    "        '''\n",
    "        n = x.size\n",
    "        p_mom = 1 - (n*sample_mean_estimator(x)) / (np.sum(x**2) - n*sample_mean_estimator(x)**2)\n",
    "        return p_mom\n",
    "\n",
    "    def k_mom(x):\n",
    "        ''' Returns the method of moments estimator for k\n",
    "            Paramters:\n",
    "                x ((n, ) np.ndarray): the sample\n",
    "            Retunrs: \n",
    "                p_mom: (float): the method of moments estimator for k\n",
    "        '''\n",
    "\n",
    "        return (sample_mean_estimator(x) * (1 - p_mom(x))) / (p_mom(x))\n",
    "\n",
    "    #we now need to find the zeros of the derivatives we took.\n",
    "    #To do this we can use Newton's Method with the intial points as the\n",
    "    #Method of moments estimator. \n",
    "\n",
    "\n",
    "    #get initial guess for k\n",
    "    k_init = k_mom(x)\n",
    "    #get intial guess for p\n",
    "    p_init = p_mom(x)\n",
    "\n",
    "    #define the function from the derivatives we took in the hw\n",
    "    n = x.size\n",
    "    μ = sample_mean_estimator(x)\n",
    "    func = lambda p, k: anp.array( [ n*μ/p - n*k/(1-p), n*anp.log(1-p) + n * digamma + anp.sum(digamma(x + k))  ] ) \n",
    "    #define jacobian \n",
    "    f_jac = jacobian(func)\n",
    "    #get initial guess\n",
    "    x0 = np.array([p_init, k_init])\n",
    "    #comparison array and tolerance\n",
    "    x1 = np.zeros()\n",
    "    tol = 1e-10\n",
    "    #while loop\n",
    "    while np.linalg.norm(x0-x1) > tol:\n",
    "        #calculae next iteration\n",
    "        temp = x0 - np.linalg.inv(f_jac(x0[0], x0[-1]))*func(x0[0], x0[1])\n",
    "        x0 = x1.copy()\n",
    "        x1 = temp.copy()\n",
    "\n",
    "    #return desired values\n",
    "    return x1[0], x1[-1]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3787f4cb79ecec69b787db7199d8bde05c4992db9bd29a2a965f7beb5defefb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
